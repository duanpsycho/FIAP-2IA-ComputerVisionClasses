{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aula 6 - Processamento de Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro de Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[120 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "blue = np.uint8([[[255, 0, 0]]])\n",
    "hsv = cv2.cvtColor(blue,cv2.COLOR_BGR2HSV)\n",
    "print(hsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo limiares de cor, adotando H-10, 100, 100 para limite inferior e H+10, 255, 255 para limite superior\n",
    "lower_blue = np.array([100,100,100])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "imagem = cv2.imread(\"imagens/caneta.jpg\")\n",
    "imagem = cv2.resize(imagem, (0,0), fx=0.5, fy=0.5) \n",
    "\n",
    "#Convertendo para HSV\n",
    "hsv_img = cv2.cvtColor(imagem, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Criação de máscara de cores dentro do limtie\n",
    "mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "\n",
    "# Operação AND entre a máscara e a imagem original\n",
    "res = cv2.bitwise_and(imagem, imagem, mask=mask)\n",
    "\n",
    "cv2.imshow('Imagem original', imagem)  \n",
    "cv2.imshow('Mascara', mask)\n",
    "cv2.imshow('Filtro de cor azul', res)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando o mesmo processo via Webcam\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definindo limiares de cor, adotando H-10, 100, 100 para limite inferior e H+10, 255, 255 para limite superior\n",
    "lower_blue = np.array([100,100,100])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Os filtros foram feitos com HSV, logo precisamos converter neste spaço de cores.\n",
    "        hsv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Criação de máscara de cores dentro do limtie\n",
    "        mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "\n",
    "        # Operação AND entre a máscara e a imagem original\n",
    "        res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "        cv2.imshow('Imagem original', frame)  \n",
    "        cv2.imshow('Mascara', mask)\n",
    "        cv2.imshow('Filtro de cor azul', res)\n",
    "        \n",
    "    #Tecla Enter interrompe o loop\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region of Interest = RIO\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Definindo limiares de cor, adotando H-10, 100, 100 para limite inferior e H+10, 255, 255 para limite superior\n",
    "lower_blue = np.array([100,100,100])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "upper_left = (50, 50)\n",
    "bottom_right = (300, 300)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        \n",
    "        #Rectangle marker\n",
    "        r = cv2.rectangle(frame, upper_left, bottom_right, (100, 50, 200), 5)\n",
    "        rect_img = frame[upper_left[1] : bottom_right[1], upper_left[0] : bottom_right[0]]\n",
    "        \n",
    "        hsv_img = cv2.cvtColor(rect_img, cv2.COLOR_BGR2HSV)\n",
    "        mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "        res = cv2.bitwise_and(rect_img, rect_img, mask=mask)\n",
    "        \n",
    "        frame[upper_left[1] : bottom_right[1], upper_left[0] : bottom_right[0]] = res\n",
    "\n",
    "        cv2.imshow('Imagem original', frame)  \n",
    "        \n",
    "    #Tecla Enter interrompe o loop\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastreamento de Objetos baseados em cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Obtendo o primeiro frame para posicionar o retângulo inicial\n",
    "\n",
    "cap = cv2.VideoCapture(\"videos/soccer-ita.mp4\")\n",
    "play = True\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"Extracao ROI\", frame)\n",
    "        cv2.imwrite(\"roi_sample.png\", frame)\n",
    "        break\n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolha do ROI da imagem, neste exemplo vamos focar no juiz de vermelho\n",
    "\n",
    "# A escolha do Juiz deve ser feita pelo mouse, direto na imagem\n",
    "\n",
    "fromCenter = False\n",
    "frame = cv2.imread(\"roi_sample.png\")\n",
    "r = cv2.selectROI(\"Image\", frame, fromCenter)\n",
    "roi = frame[r[1]:(r[1]+r[3]), r[0]:(r[0]+r[2])]\n",
    "track_window = r[0], r[1], r[2], r[3]\n",
    "cv2.imwrite(\"roi_sample_defined.png\", roi)\n",
    "cv2.imshow(\"Extracao ROI\", roi)\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_window = r[0], r[1], r[2], r[3]\n",
    "\n",
    "# Conversão para o espaço HSV\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Cálculo do histograma\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0,180])\n",
    "\n",
    "# Normalizar os valors para o range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Critério de término, encerrará o cálculo do centroid após 10 interações\n",
    "# ou se o centróide mover, pelo menos 1 pixel\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 20, 1)\n",
    "\n",
    "cap = cv2.VideoCapture(\"videos/soccer-ita.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Cálculo do histograma da projeção da imagem em região adjacente\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # Aplicação da técnica Mean Shift na nova região\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Incluir retângulo\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w, y+h), 255, 2)    \n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "        time.sleep(0.01)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meanshift: É o algoritmo responsável pelo deslocamento do objeto\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Configuração do retângulo para capturar as características do objeto a ser rastreado\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame = cv2.rectangle(frame, (c,r), (c+w, r+h), 255, 2)  \n",
    "        cv2.imshow(\"Extracao ROI\", frame)\n",
    "        if cv2.waitKey(1) == 13:\n",
    "            break\n",
    "\n",
    "# Recortar a região de interesse (ROI)\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Conversão para o espaço HSV\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Cálculo do histograma\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], None, [180], [0,180])\n",
    "\n",
    "# Normalizar os valors para o range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Critério de término, encerrará o cálculo do centroid após 10 interações\n",
    "# ou se o centróide mover, pelo menos 1 pixel\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Cálculo do histograma da projeção da imagem em região adjacente\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # Aplicação da técnica Mean Shift na nova região\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Incluir retângulo\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w, y+h), 255, 2)    \n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de cada célula: (50, 100, 20, 20)\n",
      "Amostras para treino 3500\n",
      "Amostras para teste 1500\n",
      "Precisão é de 93.47 %\n"
     ]
    }
   ],
   "source": [
    "# Obtendo imagem com números escritos a mão\n",
    "image = cv2.imread('imagens/digits.png')\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convertendo em imagem menor, apenas para visualização\n",
    "small = cv2.pyrDown(image)\n",
    "cv2.imshow('Digits Image', small)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de cada célula: (50, 100, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "# Dividir a imagem em 5000 células, cada uma com tamanho de 20x20\n",
    "# Resulta em um array de 4 dimensões: 50 x 100 x 20 x 20\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    "\n",
    "# Convertendo em um array do Numpy (50,100,20,20)\n",
    "x = np.array(cells)\n",
    "print (\"Shape de cada célula: \" + str(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uma saida\n",
    "cv2.imshow('Digits Image', cells[0][6])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostras para treino 3500\n",
      "Amostras para teste 1500\n"
     ]
    }
   ],
   "source": [
    "# Divisão de 70% de treinamento e 30% de testes\n",
    "# Valor -1 do reshape indica que os valores serão armazenados em uma única linha\n",
    "# Neste caso imagine que a matriz de cada imagem de 20 linhas x 20 colunas será convertido em uma única linha de 400 colunas\n",
    "train = x[:,:70].reshape(-1,400).astype(np.float32) # Size = (3500,400)\n",
    "test = x[:,70:100].reshape(-1,400).astype(np.float32) # Size = (1500,400)\n",
    "\n",
    "print(\"Amostras para treino \" + str(len(train)))\n",
    "print(\"Amostras para teste \" + str(len(test)))\n",
    "\n",
    "# Labels para dados de teste\n",
    "k = [0,1,2,3,4,5,6,7,8,9]\n",
    "      \n",
    "train_labels = np.repeat(k,350)[:,np.newaxis]\n",
    "test_labels = np.repeat(k,150)[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisão é de 93.47 %\n"
     ]
    }
   ],
   "source": [
    "# Inicializando o KNN, treinando os dados e configurando com k (mínimo de vizinhos) = 3\n",
    "knn = cv2.ml.KNearest_create()\n",
    "knn.train(train, cv2.ml.ROW_SAMPLE, train_labels)\n",
    "ret, result, neighbors, distance = knn.findNearest(test, k=3)\n",
    "\n",
    "# Obtendo a validação do treino, comparando os resultados com os dados de teste\n",
    "matches = result == test_labels\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = 100 * correct/result.size\n",
    "\n",
    "print(\"Precisão é de %.2f\" % accuracy + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando arquivo para identificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def centroid_contour(contour):\n",
    "    # Obtem centróide X\n",
    "    M = cv2.moments(contour)\n",
    "    cx = int(M[\"m10\"]/M[\"m00\"])\n",
    "    return cx\n",
    "\n",
    "def make_square(image):\n",
    "    # Transforma uma imagem no formato quadrado (dimensões iguais)\n",
    "    black = [0,0,0]\n",
    "    img_dim = image.shape\n",
    "    height = img_dim[0]\n",
    "    width = img_dim[1]\n",
    "\n",
    "    if (height == width):\n",
    "        square = not_square\n",
    "        return square\n",
    "    else:\n",
    "        doublesize = cv2.resize(image,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n",
    "        height = height * 2\n",
    "        width = width * 2\n",
    "        if (height > width):\n",
    "            pad = int((height - width)/2)\n",
    "            doublesize_square = cv2.copyMakeBorder(doublesize,0,0,pad,pad,cv2.BORDER_CONSTANT,value=black)\n",
    "        else:\n",
    "            pad = int((width - height)/2)\n",
    "            doublesize_square = cv2.copyMakeBorder(doublesize,pad,pad,0,0, cv2.BORDER_CONSTANT,value=black)\n",
    "    \n",
    "    doublesize_square_dim = doublesize_square.shape\n",
    "    return doublesize_square\n",
    "\n",
    "def resize_dimension(dimensions, image):\n",
    "    # Redimensionar imagem dada uma dimensão\n",
    "    black = [0,0,0]\n",
    "    buffer_pix = 4\n",
    "    dimensions  = dimensions - buffer_pix\n",
    "    squared = image\n",
    "    r = float(dimensions) / squared.shape[1]\n",
    "    dim = (dimensions, int(squared.shape[0] * r))\n",
    "    resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "    img_dim2 = resized.shape\n",
    "    height_r = img_dim2[0]\n",
    "    width_r = img_dim2[1]\n",
    "    \n",
    "    if (height_r > width_r):\n",
    "        resized = cv2.copyMakeBorder(resized,0,0,0,1,cv2.BORDER_CONSTANT,value=black)\n",
    "    if (height_r < width_r):\n",
    "        resized = cv2.copyMakeBorder(resized,1,0,0,0,cv2.BORDER_CONSTANT,value=black)\n",
    "    \n",
    "    p = 2\n",
    "    ReSizedImg = cv2.copyMakeBorder(resized,p,p,p,p,cv2.BORDER_CONSTANT,value=black)\n",
    "    img_dim = ReSizedImg.shape\n",
    "    height = img_dim[0]\n",
    "    width = img_dim[1]\n",
    "    \n",
    "    return ReSizedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Numero identificado 12345\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Carregando imagem para detecção\n",
    "image = cv2.imread('imagens/numeros.png')\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow(\"Imagem Original\", image)\n",
    "cv2.imshow(\"Imagem Escala de Cinza\", gray)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Suavizando a imagem para posterior detecção de borda por Canny \n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "cv2.imshow(\"Suavizada\", blurred)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "edged = cv2.Canny(blurred, 30, 150)\n",
    "cv2.imshow(\"Imagem Vazada para Detecao de Contornos\", edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Encontrando contornos\n",
    "im2, contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Ordenando contornos pela coordenada X\n",
    "contours = sorted(contours, key = centroid_contour, reverse = False)\n",
    "\n",
    "full_number = []\n",
    "\n",
    "for c in contours:\n",
    "    # Para cada contorno, desenhar um retângulo para capturar a escrita\n",
    "    (x, y, w, h) = cv2.boundingRect(c)    \n",
    "    cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "    cv2.imshow(\"Contornos\", image)\n",
    "    \n",
    "    # Validando tamanho da imagem\n",
    "    if w >= 5 and h >= 25:\n",
    "        roi = blurred[y:y + h, x:x + w]\n",
    "        ret, roi = cv2.threshold(roi, 127, 255,cv2.THRESH_BINARY_INV)\n",
    "        squared = make_square(roi)\n",
    "        \n",
    "        cv2.imshow(\"Imagem redimencionada\", squared)\n",
    "        cv2.waitKey(0) \n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        final = resize_dimension(20, squared)\n",
    "        cv2.imshow(\"Imagem Padronizada 20 x 20\", final)\n",
    "        \n",
    "        final_array = final.reshape((1,400))\n",
    "        final_array = final_array.astype(np.float32)\n",
    "        \n",
    "        ret, result, neighbours, dist = knn.findNearest(final_array, k=1)\n",
    "        number = str(int(float(result[0])))\n",
    "        full_number.append(number)\n",
    "        \n",
    "        # Retângulo em volta do dígito e número identificado\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(image, number, (x , y + 155), cv2.FONT_ITALIC, 2, (120, 200, 210), 3)\n",
    "        print(number)\n",
    "         \n",
    "cv2.imshow(\"Imagem Final com Digitos\", image)\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()\n",
    "print (\"Numero identificado \" + ''.join(full_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconhecimento Facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Extrator de faces\n",
    "def face_extractor(img):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.2, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "\n",
    "# Coletar 100 exemplos de um determinado rosto\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"Amostra\", frame)\n",
    "        if face_extractor(frame) is not None:\n",
    "            count += 1\n",
    "            face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            file_name_path = 'imagens/faces/michel/' + str(count) + '.jpg'\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "\n",
    "            # Put count on images and display live count\n",
    "            cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "            cv2.imshow('Rosto Normalizado', face)\n",
    "\n",
    "        if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Coleta de amostras completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Carregando exemplos de arquivos previamente coletados\n",
    "data_path = 'imagens/faces/michel/'\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "training_data, labels = [], []\n",
    "\n",
    "# Lendo as imagens e associando a um label\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    training_data.append(images)\n",
    "    labels.append(0)\n",
    "    \n",
    "# Para mais de um rosto\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    training_data.append(images)\n",
    "    labels.append(1) # Alterando o indice\n",
    "\n",
    "# Criando uma matriz da lista de labels\n",
    "labels = np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "# Treinamento do modelo\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "#model = cv2.face.EigenFaceRecognizer_create()\n",
    "model.train(training_data, labels)\n",
    "\n",
    "print(\"Modelo treinado com sucesso.\")\n",
    "\n",
    "persons = {0: \"Michel\", 1: \"Duan\"} #adiciona na chave a qntd de classificações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do Modelo\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.1, 5)\n",
    "    if faces is ():\n",
    "        return img, [], 0, 0\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (200, 200))\n",
    "    return img, roi, x, y\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    image, face, x, y = face_detector(frame)\n",
    "    \n",
    "    try:\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        results = model.predict(face)\n",
    "        \n",
    "        print(results)\n",
    "        \n",
    "        if x > 0:\n",
    "            display_string = \"Dist. \" + str(int(results[1])) + ' ' + persons[results[0]] \n",
    "            cv2.putText(image, display_string, (x, y-20), cv2.FONT_HERSHEY_DUPLEX, 1, (255,120,150), 2)\n",
    "\n",
    "        if int(results[1]) < 40:\n",
    "            cv2.putText(image, \"Reconhecido com sucesso\", (x, y-50), cv2.FONT_HERSHEY_DUPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognition', image)\n",
    "        else:\n",
    "            cv2.putText(image, \"Nao reconhecido\", (250, 450), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image)\n",
    "    except:\n",
    "        cv2.putText(image, \"Rosto nao identificado\", (220, 120) , cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"Nao reconhecido\", (250, 450), cv2.FONT_HERSHEY_DUPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio 1 \n",
    "Dada uma base de dados de imagens de celebridades na pasta \"5-celebrity-faces-dataset“ do repositório, colete os dados de cada celebridade identificado por meio do seu nome, por exemplo \"ben_afflek\" e use a pasta \"train\" para treinar um modelo de classificador de rostos e valide os resultados com outras amostras que estão na pasta \"val\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Extrator de faces\n",
    "def face_extractor(img):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.2, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes = [\"ben_afflek\", \"elton_john\", \"jerry_seinfeld\", \"madonna\"]\n",
    "persons = {0: \"Ben\", 1: \"Elton\", 2: \"Jerry\", 3: \"Madonna\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_imagens(nome):\n",
    "        list_files = []\n",
    "        i = 1\n",
    "        no_face = 0\n",
    "        total = 0\n",
    "        for file in glob.glob(\"imagens/desafio_1/\"+nome+\"/*\"):\n",
    "            total+=1\n",
    "            image = cv2.imread(file)\n",
    "            cv2.imshow(\"teste\",image)\n",
    "            face = face_extractor(image)\n",
    "            if face is not None:\n",
    "                image_norm = cv2.resize(face, (200, 200))\n",
    "                image_norm = cv2.cvtColor(image_norm, cv2.COLOR_BGR2GRAY)\n",
    "                file_name_path = \"imagens/desafio_1/\"+nome+\"/norm/\"+str(i)+\".png\"\n",
    "                cv2.imwrite(file_name_path, image_norm)\n",
    "                list_files.append(file_name_path)\n",
    "                i += 1\n",
    "            else:\n",
    "                no_face += 1\n",
    "        print(\"Faces localizadas \"+ str(i) + \"/\" + str(total))\n",
    "        return list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinamento():\n",
    "    training_data, labels = [], []\n",
    "\n",
    "    for idx, nome in enumerate(nomes):\n",
    "        print(\"Processando \"+nome)\n",
    "        files = normalizar_imagens(nome)\n",
    "\n",
    "        # Lendo as imagens e associando a um label\n",
    "        for i, file in enumerate(files):\n",
    "            images = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "            training_data.append(np.asarray(images, dtype=np.uint8))\n",
    "            labels.append(idx)\n",
    "\n",
    "    # Criando uma matriz da lista de labels\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    return training_data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NECESSÁRIO CORRIGIR O EXERCICIO, DURANTE A AULA OS CODIGOS NAO ESTAVAM CORRETOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo - Detecção automática com Redes Neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1ee48c532453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from utils import *\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdarknet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Configurações na rede neural YOLOv3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\darknet.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from darknet import Darknet\n",
    "\n",
    "\n",
    "# Configurações na rede neural YOLOv3\n",
    "cfg_file = 'cfg/yolov3.cfg'\n",
    "m = Darknet(cfg_file)\n",
    "# Pesos pré-treinados\n",
    "weight_file = 'weights/yolov3.weights'\n",
    "m.load_weights(weight_file)\n",
    "# Rótulos de classes\n",
    "namesfile = 'data/coco.names'\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topologia da rede neural da YOLOv3\n",
    "m.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho da figura\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregando imagem para classificar\n",
    "img = cv2.imread('./imagens/surf.jpg')\n",
    "\n",
    "# Convertendo para o espaço de cores RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionando imagem para ser compatível com a primeira camada da rede neural  \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Exibição das imagens\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Resized Image')\n",
    "plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patamar de NMS (Non-Maximum Supression)\n",
    "# Ajuste de sensibilidade de imagens com baixa luminosidade\n",
    "nms_thresh = 0.6\n",
    "\n",
    "# Patamar do IOU (Intersect of Union), indicador se o retângulo \n",
    "# de identificação de imagem foi adequadamente desenhado\n",
    "iou_thresh = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo tamnaho do gráfico\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregar imagem para classificação\n",
    "img = cv2.imread('imagens/surf.jpg')\n",
    "\n",
    "# Conversão para o espaço RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionamento para adatapção da primeira camada da rede neural \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Deteteção de objetos na imagem\n",
    "boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n",
    "\n",
    "# Objetos encontrados e nível de confiança\n",
    "print_objects(boxes, class_names)\n",
    "\n",
    "# Desenho no gráfico com os regângulos e rótulos\n",
    "plot_boxes(original_image, boxes, class_names, plot_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_objects(boxes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio 2 \n",
    "Ajuste o classificador baseado na implementação YOLOv3 para analisar a imagem abaixo (“imagens/oldman.jpg”) de modo a identificar o máximo de objetos em cena.  \n",
    "Após regular a detecção de objetos (utilize o método list_objects da classe utils.py), liste os objetos detectados e a quantidade deles, realizando uma auditoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6bcd94b596ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdarknet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Configurações na rede neural YOLOv3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from darknet import Darknet\n",
    "\n",
    "\n",
    "# Configurações na rede neural YOLOv3\n",
    "cfg_file = 'cfg/yolov3.cfg'\n",
    "m = Darknet(cfg_file)\n",
    "# Pesos pré-treinados\n",
    "weight_file = 'weights/yolov3.weights'\n",
    "m.load_weights(weight_file)\n",
    "# Rótulos de classes\n",
    "namesfile = 'data/coco.names'\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho da figura\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregando imagem para classificar\n",
    "img = cv2.imread('./imagens/surf.jpg')\n",
    "\n",
    "# Convertendo para o espaço de cores RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionando imagem para ser compatível com a primeira camada da rede neural  \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Exibição das imagens\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_image)\n",
    "plt.subplot(122)\n",
    "plt.title('Resized Image')\n",
    "plt.imshow(resized_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patamar de NMS (Non-Maximum Supression)\n",
    "# Ajuste de sensibilidade de imagens com baixa luminosidade\n",
    "nms_thresh = 0.1\n",
    "\n",
    "# Patamar do IOU (Intersect of Union), indicador se o retângulo \n",
    "# de identificação de imagem foi adequadamente desenhado\n",
    "iou_thresh = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = list_objects(boxes, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_objects = Counter(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_objects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
